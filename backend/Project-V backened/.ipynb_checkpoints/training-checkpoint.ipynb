{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b34492d-5788-4b66-9e15-149f78624018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "tar=6\n",
    "path='./dataset/'\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    files = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']), tar)\n",
    "    return files, targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset(path)\n",
    "\n",
    "test_files=train_files\n",
    "test_targets = train_targets\n",
    "\n",
    "# get the burn classes\n",
    "# We only take the characters from a starting position to remove the path\n",
    "#burn_classes = [item[11:-1] for item in sorted(glob(path))]\n",
    "burn_classes = [item[10:-1] for item in sorted(glob(\"./dataset/*/\"))]\n",
    "# print statistics about the dataset\n",
    "print('There are %d total categories.' % len(burn_classes))\n",
    "print(burn_classes)\n",
    "print('There are %s total  images.\\n' % len(np.hstack([train_files, test_files])))\n",
    "print('There are %d training images.' % len(train_files))\n",
    "print('There are %d test images.'% len(test_files))\n",
    "\n",
    "for file in train_files: assert('.DS_Store' not in file)\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "# Note: modified these two functions, so that we can later also read the inception tensors which \n",
    "# have a different format \n",
    "def path_to_tensor(img_path, width=224, height=224):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(width, height))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (width, heigth, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, width, height, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths, width=224, height=224):\n",
    "    list_of_tensors = [path_to_tensor(img_path, width, height) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "import keras\n",
    "import timeit\n",
    "\n",
    "# graph the history of model.fit\n",
    "def show_history_graph(history):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_width, img_height = 224, 224\n",
    "batch_size = 4\n",
    "epoch=100\n",
    "\n",
    "########\n",
    "\n",
    "img_width, img_height = img_width, img_height\n",
    "batch_size = 32\n",
    "samples_per_epoch = 40\n",
    "validation_steps = 40\n",
    "nb_filters1 = 32\n",
    "nb_filters2 = 64\n",
    "conv1_size = 3\n",
    "conv2_size = 3\n",
    "pool_size = 3\n",
    "lr = 0.0004\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D\n",
    "from tensorflow.keras import callbacks\n",
    "import time\n",
    "#input_shape=(img_width, img_height,3)\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(nb_filters1, conv1_size, conv1_size, padding='same', input_shape=(img_width, img_height, 3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "\n",
    "model.add(Convolution2D(nb_filters2, conv2_size, conv2_size, padding='same'))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(tar, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=lr),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "hist=model.fit(train_tensors, train_targets ,validation_split=0.3, epochs=epoch, batch_size=10)\n",
    "\n",
    "show_history_graph(hist)\n",
    "test_loss, test_acc = model.evaluate(train_tensors, train_targets)\n",
    "\n",
    "y_pred=model.predict(train_tensors)\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,f1_score,recall_score\n",
    "cm = confusion_matrix(np.argmax(train_targets, axis=1),np.argmax(y_pred, axis=1))\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "# Calculate ROC curve from y_test and pred\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr ,tn, thresholds = roc_curve(np.argmax(test_targets,axis=1)>=1,np.argmax(y_pred,axis=1)>=1)\n",
    "accuracycnn = accuracy_score(np.argmax(test_targets, axis=1)>=1,np.argmax(y_pred, axis=1)>=1)\n",
    "print(cm)\n",
    "print('Accuracy=' +str(accuracycnn*100))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model.save('color_trained_modelDNN.h5')\n",
    "model.save('trained_model_DNN1.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
